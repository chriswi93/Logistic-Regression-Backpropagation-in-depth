<h1>Logistic Regression - How to compute gradients of binary cross entropy loss</h1>
<p align="left" style="font-size:13px;"><b>Christoph Winkler</b><br>
<i>M. Sc. Business Information Systems, Data Scientist.<br>
Generative Modeling is simply about modeling "How the world could be" and not necessarily "How the world actually is".</i></p>

Logistic regression is one of the most popular machine learning models for classification. Actually logistic regression is a special case of a neural networks and is therefore an appropriate introduction to learn how backpropagation in neural networks works. In this article the theoretical foundation on how to derive the equations for optimization of the model parameters is discussed. In particular the derivation of the equations for gradient descent is shown. 

<h2>Model</h2>
Basically logistic regression is a classification model that tries to learn from data x if a certain data point belongs to class zero or  one. It is also important to note that in contrast to linear regression logistic regression can only predict probabilities between zero and one and therefore can only be used for classification and not regression. In comparison to neural networks also has multiple input nodes which represent the features of a data set. Additionally, logistic regression has no hidden layer and only a single output node with a sigmoid activation function. The sigmoid activation function is applied to transform logits z without any upper or lower bound into probabilities g. If the weight tensor w that represents the connection between input nodes and output node takes on a large activation for a certain input data point, the probability for class one is also quite large.

<p align="center">
<img src="logistic_regression.png"></img>
</p>

<h2>Loss and Optimization</h2>
The loss that is minimized during training is known as binary cross entropy. It decreases if the predicted values of the class labels get closer to the true class labels and is therefore an appropriate method to measure the learning progress and convergence of the model.
<p align="center">
<img src="loss.png"></img>
</p>

In order to improve the performance of the model we have to change the weights w and the bias term b. The optimization technique gradient descent helps us to determine in which direction we have to change the trainable variables. In particular gradient descent computes the gradients of the variables to determine the direction of the steepest descent of the loss function. The learning rate equals the step size on the loss function in each iteration. If the learning rate is chosen very large the learning speed increases, but the algorithm might jump over the global minimum and there never converges. In contrast if the learning rate is chosen very small the learning speed is very low and the computational efforts for large data sets increase. Therefore, it is important to choose the learning rate with caution. In order to use gradient descent we have to derive the equations for gradients in our the loss function for each  trainable variable. This is best explained using a so called computation graph. Computation graphs are widely used in mathematical applications to discover dependencies which helps us to derive the equations for the gradients.

<h2>Computation Graph</h2>
In general the computation graph in the context of optimization is the application of the chain rule in calculus. 

<p align="center">
<a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{120}&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L}{\partial&space;g}\frac{\partial&space;g}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L}{\partial&space;g}\frac{\partial&space;g}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w}" title="\frac{\partial L}{\partial w} = \frac{\partial L}{\partial g}\frac{\partial g}{\partial z}\frac{\partial z}{\partial w}" /></a>
</p>

COMPUTATION GRAPH

<h2>Derivatives for Gradient Descent</h2>

<a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{120}&space;\newline&space;L&space;=&space;-\sum_{i}^{m}&space;y\log(g)&space;&plus;&space;(1&space;-&space;y)\log(1-g)&space;\newline&space;g&space;=&space;\sigma&space;(z)&space;=&space;\frac{1}{1&plus;e^{-z}}&space;\newline&space;z&space;=&space;wx^{T}&space;&plus;&space;b&space;\newline\newline&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L}{\partial&space;g}\frac{\partial&space;g}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\newline&space;L&space;=&space;-\sum_{i}^{m}&space;y\log(g)&space;&plus;&space;(1&space;-&space;y)\log(1-g)&space;\newline&space;g&space;=&space;\sigma&space;(z)&space;=&space;\frac{1}{1&plus;e^{-z}}&space;\newline&space;z&space;=&space;wx^{T}&space;&plus;&space;b&space;\newline\newline&space;\frac{\partial&space;L}{\partial&space;w}&space;=&space;\frac{\partial&space;L}{\partial&space;g}\frac{\partial&space;g}{\partial&space;z}\frac{\partial&space;z}{\partial&space;w}" title="\newline L = -\sum_{i}^{m} y\log(g) + (1 - y)\log(1-g) \newline g = \sigma (z) = \frac{1}{1+e^{-z}} \newline z = wx^{T} + b \newline\newline \frac{\partial L}{\partial w} = \frac{\partial L}{\partial g}\frac{\partial g}{\partial z}\frac{\partial z}{\partial w}" /></a>

<a href="https://www.codecogs.com/eqnedit.php?latex=\dpi{120}&space;\newline&space;\frac{\partial&space;L}{\partial&space;g}&space;=&space;\sum_{i}^{m}&space;\frac{1-y}{1-g}&space;-&space;\frac{y}{g}&space;\newline\newline\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{e^{-z}}{(e^{-z}&space;&plus;&space;1)^2}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\frac{e^{-z}}{e^{-z}&space;&plus;&space;1}&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\frac{(e^{-z}&space;&plus;&space;1)&space;-&space;1}{e^{-z}&space;&plus;&space;1}&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\left&space;(\frac{e^{-z}&space;&plus;&space;1}{e^{-z}&space;&plus;&space;1}-\frac{1}{e^{-z}&space;&plus;&space;1}&space;\right&space;)&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\left&space;(1-\frac{1}{e^{-z}&space;&plus;&space;1}&space;\right&space;)&space;=&space;\sigma&space;(z)(1-\sigma&space;(z))&space;\newline\newline\newline&space;\frac{\partial&space;z}{\partial&space;w}&space;=&space;x^{T}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\dpi{120}&space;\newline&space;\frac{\partial&space;L}{\partial&space;g}&space;=&space;\sum_{i}^{m}&space;\frac{1-y}{1-g}&space;-&space;\frac{y}{g}&space;\newline\newline\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{e^{-z}}{(e^{-z}&space;&plus;&space;1)^2}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\frac{e^{-z}}{e^{-z}&space;&plus;&space;1}&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\frac{(e^{-z}&space;&plus;&space;1)&space;-&space;1}{e^{-z}&space;&plus;&space;1}&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\left&space;(\frac{e^{-z}&space;&plus;&space;1}{e^{-z}&space;&plus;&space;1}-\frac{1}{e^{-z}&space;&plus;&space;1}&space;\right&space;)&space;\newline&space;\frac{\partial&space;g}{\partial&space;z}&space;=&space;\frac{1}{e^{-z}&space;&plus;&space;1}\left&space;(1-\frac{1}{e^{-z}&space;&plus;&space;1}&space;\right&space;)&space;=&space;\sigma&space;(z)(1-\sigma&space;(z))&space;\newline\newline\newline&space;\frac{\partial&space;z}{\partial&space;w}&space;=&space;x^{T}" title="\newline \frac{\partial L}{\partial g} = \sum_{i}^{m} \frac{1-y}{1-g} - \frac{y}{g} \newline\newline\newline \frac{\partial g}{\partial z} = \frac{e^{-z}}{(e^{-z} + 1)^2} = \frac{1}{e^{-z} + 1}\frac{e^{-z}}{e^{-z} + 1} \newline \frac{\partial g}{\partial z} = \frac{1}{e^{-z} + 1}\frac{(e^{-z} + 1) - 1}{e^{-z} + 1} \newline \frac{\partial g}{\partial z} = \frac{1}{e^{-z} + 1}\left (\frac{e^{-z} + 1}{e^{-z} + 1}-\frac{1}{e^{-z} + 1} \right ) \newline \frac{\partial g}{\partial z} = \frac{1}{e^{-z} + 1}\left (1-\frac{1}{e^{-z} + 1} \right ) = \sigma (z)(1-\sigma (z)) \newline\newline\newline \frac{\partial z}{\partial w} = x^{T}" /></a>


